{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86bc5cce-c20d-4936-9da5-e68aa381f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing modules\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as df\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.signal import freqz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6640fbc4-4d97-4630-92d5-7fe63ee98ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0101T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0103T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 469010  =      0.000 ...  1876.040 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0201T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0202T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0203T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 464154  =      0.000 ...  1856.616 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0301T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0302T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0303T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 464674  =      0.000 ...  1858.696 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0401T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0402T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 696265  =      0.000 ...  2785.060 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0403T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 468558  =      0.000 ...  1874.232 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0501T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0502T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 696265  =      0.000 ...  2785.060 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0503T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 461874  =      0.000 ...  1847.496 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0601T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0602T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0603T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 470390  =      0.000 ...  1881.560 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0701T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0702T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0703T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 466726  =      0.000 ...  1866.904 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0801T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 787728  =      0.000 ...  3150.912 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0802T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0803T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 474834  =      0.000 ...  1899.336 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0901T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0902T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 604802  =      0.000 ...  2419.208 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0903T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 460730  =      0.000 ...  1842.920 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:35: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lenovo\\Desktop\\JU\\2b_dataset\\B0102T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG:C3, EEG:Cz, EEG:C4, EOG:ch01, EOG:ch02, EOG:ch03\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_21188\\2075795585.py:41: RuntimeWarning: Highpass cutoff frequency 100.0 is greater than lowpass cutoff frequency 0.5, setting values to 0 and Nyquist.\n",
      "  raw_data1 =mne.io.read_raw_gdf(\"B0102T.gdf\")\n"
     ]
    }
   ],
   "source": [
    "#dropping the eog channels as we will be working only on 22 eeg channels\n",
    "file_paths_1 = [\n",
    "    'B0101T.gdf',\n",
    "    \n",
    "    'B0103T.gdf',\n",
    "    'B0201T.gdf',\n",
    "    'B0202T.gdf',\n",
    "    'B0203T.gdf',\n",
    "    'B0301T.gdf',\n",
    "    'B0302T.gdf',\n",
    "    'B0303T.gdf',\n",
    "    'B0401T.gdf',\n",
    "    'B0402T.gdf',\n",
    "    'B0403T.gdf',\n",
    "    'B0501T.gdf',\n",
    "    'B0502T.gdf',\n",
    "    'B0503T.gdf',\n",
    "    'B0601T.gdf',\n",
    "    'B0602T.gdf',\n",
    "    'B0603T.gdf',\n",
    "    'B0701T.gdf',\n",
    "    'B0702T.gdf',\n",
    "    'B0703T.gdf',\n",
    "    'B0801T.gdf',\n",
    "    'B0802T.gdf',\n",
    "    'B0803T.gdf',\n",
    "    'B0901T.gdf',\n",
    "    'B0902T.gdf',\n",
    "    'B0903T.gdf'\n",
    "]\n",
    "\n",
    "raw = []\n",
    "\n",
    "for file_path in file_paths_1:\n",
    "    raw_data = mne.io.read_raw_gdf(file_path,preload=True, eog=['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n",
    "    \n",
    "    p=raw_data.drop_channels(['EOG:ch01', 'EOG:ch02', 'EOG:ch03'])\n",
    "    raw.append(p)\n",
    "    \n",
    "#This file has no EOG channel\n",
    "raw_data1 =mne.io.read_raw_gdf(\"B0102T.gdf\")\n",
    "raw.append(raw_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f54b6aa-abe4-46b4-98ea-4594f05b399c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-10-25 09:35:11 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-11-30 10:44:29 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-10-25 11:59:13 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-10-31 10:47:27 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-12-06 08:58:14 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-11-02 15:37:16 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-11-17 10:12:22 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-12-13 13:54:33 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-11-03 11:29:51 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-11-07 12:38:31 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-11-30 12:28:35 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-10-24 13:03:40 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-11-02 12:55:35 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-11-29 11:09:04 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-10-28 10:03:22 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-11-02 10:19:07 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-12-02 10:59:18 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-11-22 10:06:12 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-11-24 10:31:57 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-12-22 09:10:00 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-11-21 09:19:10 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-12-02 10:18:33 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2006-01-20 10:16:58 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-11-29 14:19:32 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-12-06 10:16:49 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n",
      "<Info | 7 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4\n",
      " chs: 3 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 125.0 Hz\n",
      " meas_date: 2005-12-09 09:20:50 UTC\n",
      " nchan: 3\n",
      " projs: []\n",
      " sfreq: 250.0 Hz\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,26):\n",
    "    print(raw[i].info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053fbd05-3efa-4e6d-bb00-44e752fac230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n"
     ]
    }
   ],
   "source": [
    "#Annotation object for annotating segments of raw data.\n",
    "for i in range(0,26):\n",
    "    raw[i].annotations\n",
    "    events=mne.events_from_annotations(raw[i])\n",
    "    #printing the 1st index of the events which contains the overall details of the file\n",
    "    events[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23e287c-a528-4cbb-80d9-490e479633cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a event dictionary\n",
    "event_dict={\n",
    " 'rejected trial':1,\n",
    " 'horizontal eye movement':2,\n",
    " 'vertical eye movement':3,\n",
    " 'eye rotation':4,\n",
    " 'eye blinks':5,\n",
    " 'eye open':6,\n",
    " 'eye close':7,\n",
    " 'start of a new run':8,\n",
    " 'start of a trial':9,\n",
    " 'class 1':10,\n",
    " 'class 2':11,\n",
    " 'BCI feedback':12,\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95693a05-4662-4c75-821d-80864a56a9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770']\n",
      "Used Annotations descriptions: ['1023', '1077', '1078', '1079', '1081', '276', '277', '32766', '768', '769', '770', '781']\n"
     ]
    }
   ],
   "source": [
    "#Annotation object for annotating segments of raw data.\n",
    "for i in range(0,26):\n",
    "    raw[i].annotations\n",
    "    events=mne.events_from_annotations(raw[i])\n",
    "    #printing the 1st index of the events which contains the overall details of the file\n",
    "    events[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c53a8766-0f7d-4bef-a42c-da91280f3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the rawinfo(information of each file)\n",
    "rawinfo=[]\n",
    "for i in range(0,26):\n",
    "    rawinfo=np.append(rawinfo,raw[i].info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d64e099a-8014-46a7-8825-311670455bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "#taking classA of 27 subjects\n",
    "classA=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    epochs = mne.Epochs(raw[i], events[0], event_id=[10], tmin=0.0, tmax=7.5, baseline=(0, 0), preload=True)\n",
    "    classA[i] = epochs.copy().crop(tmin=3.0, tmax=7.5).get_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14700a5f-07f0-4f22-aa87-c608ebca6e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 80 events and 1876 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "#taking classA of 27 subjects\n",
    "classB=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    epochs = mne.Epochs(raw[i], events[0], event_id=[11], tmin=0.0, tmax=7.5, baseline=(0, 0), preload=True)\n",
    "    classB[i] = epochs.copy().crop(tmin=3.0, tmax=7.5).get_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7e6a284-3066-487f-9feb-400f6c9c2233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "#converting the classA to epochsArray\n",
    "A=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    A[i]=mne.EpochsArray(classA[i],rawinfo[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f95bee4-38c6-43ab-b256-a735322e03b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Not setting metadata\n",
      "80 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "#converting the classA to epochsArray\n",
    "B=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    B[i]=mne.EpochsArray(classB[i],rawinfo[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45d0b7f4-ba8e-43d0-9dd2-c846679ba076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling\n",
    "for i in range(0,26):\n",
    "    min_valA = np.min(A[i], axis=2, keepdims=True)\n",
    "    max_valA = np.max(A[i], axis=2, keepdims=True)\n",
    "    A[i] = (A[i] - min_valA) / (max_valA - min_valA)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61523a03-43fa-4ef4-991a-fdfdb7824d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling\n",
    "for i in range(0,8):\n",
    "    min_valB = np.min(B[i], axis=2, keepdims=True)\n",
    "    max_valB = np.max(B[i], axis=2, keepdims=True)\n",
    "    B[i] = (B[i] - min_valB) / (max_valB - min_valB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "092b1b92-ffb7-4b95-ada4-90379c1d96e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import scipy.signal as signal\\n\\n# Define the notch filter parameters\\nfs = 250  # Sampling frequency\\nnotch_freq = 50.0  # Powerline frequency in Hz\\nQ = 30.0  # Quality factor\\n\\n# Design a notch filter\\nb, a = signal.iirnotch(notch_freq, Q, fs)\\n\\n# Apply the notch filter to EEG segments\\ndef apply_notch_filter(eeg_segments):\\n    num_segments, num_channels, segment_length = eeg_segments.shape\\n    filtered_segments = np.zeros_like(eeg_segments)\\n\\n    for i in range(num_segments):\\n        for j in range(num_channels):\\n            filtered_segments[i, j] = signal.lfilter(b, a, eeg_segments[i, j])\\n            \\n    return filtered_segments\\n\\nfiltered_class_A_segments = apply_notch_filter(newA)\\nfiltered_class_B_segments = apply_notch_filter(newB)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Notch Filter if needed\n",
    "'''import scipy.signal as signal\n",
    "\n",
    "# Define the notch filter parameters\n",
    "fs = 250  # Sampling frequency\n",
    "notch_freq = 50.0  # Powerline frequency in Hz\n",
    "Q = 30.0  # Quality factor\n",
    "\n",
    "# Design a notch filter\n",
    "b, a = signal.iirnotch(notch_freq, Q, fs)\n",
    "\n",
    "# Apply the notch filter to EEG segments\n",
    "def apply_notch_filter(eeg_segments):\n",
    "    num_segments, num_channels, segment_length = eeg_segments.shape\n",
    "    filtered_segments = np.zeros_like(eeg_segments)\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        for j in range(num_channels):\n",
    "            filtered_segments[i, j] = signal.lfilter(b, a, eeg_segments[i, j])\n",
    "            \n",
    "    return filtered_segments\n",
    "\n",
    "filtered_class_A_segments = apply_notch_filter(newA)\n",
    "filtered_class_B_segments = apply_notch_filter(newB)'''\n",
    "\n",
    "# Now the EEG segments for both classes have the 50Hz powerline frequency removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baf5ee24-eb7c-4add-9a0b-412998a82409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the bandpass filter function\n",
    "def butter_bandpass_filter(signal, lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut/nyq\n",
    "    high = highcut/nyq\n",
    "    b,a = butter(order, [low, high], btype='band')\n",
    "    y = lfilter(b, a, signal, axis=-1)  \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29f8ff07-4dc8-4bda-90ed-9f61ab4d5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delta bandpass filter from class A\n",
    "def read_da(i):\n",
    "    fs = 250\n",
    "    lowcut = 0.5\n",
    "    highcut = 3.99\n",
    "    delta= butter_bandpass_filter(A[i], lowcut, highcut, fs, order=5)\n",
    "    mean_filtered_data = np.mean(delta, axis=-1)\n",
    "    return mean_filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "940018f4-9e74-407e-ae1b-d4374179d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function for delta of classA separately for each files\n",
    "deltaA=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    deltaA[i]=read_da(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed3af64c-b5cf-43db-845f-1ddc266be5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delta bandpass filter from class B\n",
    "def read_db(i):\n",
    "    fs = 250\n",
    "    lowcut = 0.5\n",
    "    highcut = 3.99\n",
    "    delta= butter_bandpass_filter(B[i], lowcut, highcut, fs, order=5)\n",
    "    delta = np.mean(delta, axis=-1)\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d4cbf2e-127f-4833-acf7-0341f4b29355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the shape of the delta for classB\n",
    "deltaB=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    deltaB[i]=read_db(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf1518b7-79d2-4ff0-bb88-e72e2484d340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta bandpass filter from class A\n",
    "def read_de(i):\n",
    "    fs = 250\n",
    "    lowcut = 4\n",
    "    highcut = 7.99\n",
    "    theta= butter_bandpass_filter(A[i], lowcut, highcut, fs, order=5)\n",
    "    theta = np.mean(theta, axis=-1)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dffd243-b517-4e7d-abd2-592dee33a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the shape of the theta for class A\n",
    "thetaA=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    thetaA[i]=read_de(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "917d45b2-0a82-469c-be59-6e5e083792cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Theta bandpass filter from class B\n",
    "def read_df(i):\n",
    "    fs = 250\n",
    "    lowcut = 4\n",
    "    highcut = 7.99\n",
    "    theta= butter_bandpass_filter(B[i], lowcut, highcut, fs, order=5)\n",
    "    theta = np.mean(theta, axis=-1)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a9f5f0f-b968-4ce7-adb4-0350628f8e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the shape of the theta for class B\n",
    "thetaB=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    thetaB[i]=read_df(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4323dd95-453e-4be2-8a9c-0da4cc67f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alpha bandpass filter from class A\n",
    "def read_di(i):\n",
    "    fs = 250\n",
    "    lowcut = 8.01\n",
    "    highcut = 11.99\n",
    "    alpha= butter_bandpass_filter(A[i], lowcut, highcut, fs, order=5)\n",
    "    alpha = np.mean(alpha, axis=-1)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65611c47-216a-45fa-ae76-0b4720f56ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the shape of the alpha for class A\n",
    "alphaA=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    alphaA[i]=read_di(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd1230a1-175c-413c-b74f-9fe18c412e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alpha bandpass filter from class B\n",
    "def read_dj(i):\n",
    "    fs = 250\n",
    "    lowcut = 8.01\n",
    "    highcut = 11.99\n",
    "    alpha= butter_bandpass_filter(B[i], lowcut, highcut, fs, order=5)\n",
    "    alpha = np.mean(alpha, axis=-1)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12834068-319b-4ef4-82d5-6ea1bdb78a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the shape of the alpha of class B\n",
    "alphaB=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    alphaB[i]=read_dj(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1838c07-79be-404f-9de5-87c102da2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beta bandpass filter from class A\n",
    "def read_dm(i):\n",
    "    fs = 250\n",
    "    lowcut = 12.01\n",
    "    highcut = 29.99\n",
    "    beta= butter_bandpass_filter(A[i], lowcut, highcut, fs, order=5)\n",
    "    beta = np.mean(beta, axis=-1)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45fe0dfa-e87b-4d24-adff-b462fa237a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the shape of the beta of class A\n",
    "betaA=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    betaA[i]=read_dm(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5dd6e45-c6f0-429f-9dce-40a99dabba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beta bandpass filter from class B\n",
    "def read_dn(i):\n",
    "    fs = 250\n",
    "    lowcut = 12.01\n",
    "    highcut = 29.99\n",
    "    beta= butter_bandpass_filter(B[i], lowcut, highcut, fs, order=5)\n",
    "    beta = np.mean(beta, axis=-1)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd783b21-76a4-40da-a6dc-dcf813609772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the shape of the beta of class B\n",
    "betaB=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    betaB[i]=read_dn(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cabf3b3-42e2-4104-9602-4f44c9792dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gamma bandpass filter from class A\n",
    "def read_dq(i):\n",
    "    fs = 250\n",
    "    lowcut = 30.01\n",
    "    highcut = 100\n",
    "    gamma= butter_bandpass_filter(A[i], lowcut, highcut, fs, order=5)\n",
    "    gamma = np.mean(gamma, axis=-1)\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "948fd139-4af6-4975-abe6-a31ab8dbd564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the shape of the gamma of class A\n",
    "gammaA=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    gammaA[i]=read_dq(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90769fe9-e91f-4f00-8927-eda7a00fa24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gamma bandpass filter from class B\n",
    "def read_dr(i):\n",
    "    fs = 250\n",
    "    lowcut = 30.01\n",
    "    highcut = 100\n",
    "    gamma= butter_bandpass_filter(B[i], lowcut, highcut, fs, order=5)\n",
    "    gamma = np.mean(gamma, axis=-1)\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60a359b3-9a93-48a6-bfdf-90892a9040e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the shape of the gamma of class B\n",
    "gammaB=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    gammaB[i]=read_dr(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7828805d-931e-42bb-ade8-3699f48b40b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_median_features(eeg_signals):\n",
    "    mean_median_features = []\n",
    "\n",
    "    for signal in eeg_signals:\n",
    "        segment_features = []\n",
    "\n",
    "        for segment in signal:\n",
    "            mean_feat = np.mean(segment)\n",
    "            median_feat = np.median(segment)\n",
    "            \n",
    "            segment_features.append(mean_feat)\n",
    "            segment_features.append(median_feat)\n",
    "        \n",
    "        mean_median_features.append(segment_features)\n",
    "\n",
    "    return np.array(mean_median_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5d7a74c-f4de-40f5-af24-cceccca59379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Hjorth parameters for each EEG segment\n",
    "def hjorth_parameters(eeg_segment):\n",
    "    activity = np.var(eeg_segment)\n",
    "    diff_signal = np.diff(eeg_segment)\n",
    "    mobility = np.var(diff_signal)\n",
    "    diff2_signal = np.diff(diff_signal)\n",
    "    complexity = np.var(diff2_signal)\n",
    "    \n",
    "    return activity, mobility, complexity\n",
    "\n",
    "# Calculate Hjorth features for all segments in the EEG signals\n",
    "def calculate_hjorth_features(eeg_signals):\n",
    "    hjorth_features = []\n",
    "\n",
    "    for signal in eeg_signals:\n",
    "        segment_features = []\n",
    "\n",
    "        for segment in signal:\n",
    "            hjorth_feats = hjorth_parameters(segment)\n",
    "            segment_features.extend(hjorth_feats)\n",
    "        \n",
    "        hjorth_features.append(segment_features)\n",
    "\n",
    "    return np.array(hjorth_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "def7ebbf-7699-4be9-999c-601dd737af91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4070568 , 0.37980493, 0.47934596, ..., 0.07075923, 0.09609868,\n",
       "       0.08749283])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8cea0c6a-4d20-4522-ae75-2a42a56e5063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(signal):\n",
    "    # Compute the histogram of the signal\n",
    "    hist, _ = np.histogram(signal, bins=256, range=(0, 256))\n",
    "    \n",
    "    # Normalize the histogram\n",
    "    hist = hist / (len(signal) * 1.0)\n",
    "    #print(hist)\n",
    "    # Calculate Shannon entropy\n",
    "    entropy = -np.sum([p * np.log2(p + np.finfo(float).eps) for p in hist if p > 0])\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def calculate_shannon_entropy_features(eeg_signals):\n",
    "    shannon_entropy_features = []\n",
    "\n",
    "    for signal in eeg_signals:\n",
    "        segment_features = []\n",
    "\n",
    "        for segment in signal:\n",
    "            entropy_feat = shannon_entropy(segment)\n",
    "            segment_features.append(entropy_feat)\n",
    "        \n",
    "        shannon_entropy_features.append(segment_features)\n",
    "\n",
    "    return np.array(shannon_entropy_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce59fac4-16d8-4ff3-b6a4-b8564e456c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "822179f3-dfa6-43db-9731-6adf750f28d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010283343991908014\n"
     ]
    }
   ],
   "source": [
    "signal=A[0][0][0]\n",
    "# Compute the histogram of the signal\n",
    "hist, _ = np.histogram(signal, bins=256, range=(0, 256))\n",
    "    \n",
    "# Normalize the histogram\n",
    "hist = hist / (len(signal) * 1.0)\n",
    "#print(hist)\n",
    "# Calculate Shannon entropy\n",
    "entropy = -np.sum([p * np.log2(p + np.finfo(float).eps) for p in hist if p > 0])\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0f928-26a3-4a4e-abdb-c4c94b30c4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d94793d-777a-4edf-b08c-6b3c5cfe1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "\n",
    "def calculate_wavelet_features(eeg_signals, wavelet='db4', level=4):\n",
    "    wavelet_features = []\n",
    "\n",
    "    for signal in eeg_signals:\n",
    "        segment_features = []\n",
    "\n",
    "        for segment in signal:\n",
    "            # Apply discrete wavelet transform\n",
    "            coeffs = pywt.wavedec(segment, wavelet, level=level)\n",
    "            \n",
    "            # Extract the approximation and detail coefficients\n",
    "            approx = coeffs[0]\n",
    "            details = np.concatenate(coeffs[1:])\n",
    "            \n",
    "            # Calculate statistical features (mean, variance, etc.) on coefficients\n",
    "            mean = np.mean(details)\n",
    "            variance = np.var(details)\n",
    "            energy = np.sum(np.square(details))\n",
    "            \n",
    "            # Append wavelet features for this segment\n",
    "            segment_features.extend([mean, variance, energy])\n",
    "        \n",
    "        wavelet_features.append(segment_features)\n",
    "\n",
    "    return np.array(wavelet_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53ca05d4-7690-453d-9761-ff74ff612d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53e481a9-4d44-4360-b959-8eaf0c04185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def calculate_skewness_kurtosis_features(eeg_signals):\n",
    "    skewness_features = []\n",
    "    kurtosis_features = []\n",
    "\n",
    "    for signal in eeg_signals:\n",
    "        segment_skewness = []\n",
    "        segment_kurtosis = []\n",
    "\n",
    "        for segment in signal:\n",
    "            # Calculate skewness and kurtosis for each segment\n",
    "            skew_val = skew(segment)\n",
    "            kurtosis_val = kurtosis(segment)\n",
    "\n",
    "            segment_skewness.append(skew_val)\n",
    "            segment_kurtosis.append(kurtosis_val)\n",
    "\n",
    "        # Append skewness and kurtosis features for this signal\n",
    "        skewness_features.append(segment_skewness)\n",
    "        kurtosis_features.append(segment_kurtosis)\n",
    "\n",
    "    return np.array(skewness_features), np.array(kurtosis_features)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming eeg_signals is a list of EEG signal segments, where each segment is a 1D numpy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b9e9165-9496-4116-ac28-66d266c2054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanmedianA=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "meanmedianB=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "hjorthA=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "hjorthB=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "shanon_A=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "shanon_B=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "wavelet_features_A=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "wavelet_features_B=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "skewnessA=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "skewnessB=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "kurtosisA=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "kurtosisB=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in range(0,26):\n",
    "    meanmedianA[i]=calculate_mean_median_features(A[i])\n",
    "    meanmedianB[i]=calculate_mean_median_features(B[i])\n",
    "    hjorthA[i] = calculate_hjorth_features(A[i])\n",
    "    hjorthB[i] = calculate_hjorth_features(B[i])\n",
    "    shanon_A[i] = calculate_shannon_entropy_features(A[i])\n",
    "    shanon_B[i] = calculate_shannon_entropy_features(B[i])\n",
    "    wavelet_features_A[i] = calculate_wavelet_features(A[i])\n",
    "    wavelet_features_B[i] = calculate_wavelet_features(B[i])\n",
    "    skewnessA[i], kurtosisA[i] = calculate_skewness_kurtosis_features(A[i])\n",
    "    skewnessB[i], kurtosisB[i] = calculate_skewness_kurtosis_features(B[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5f1c5e49-8e7e-4267-88b1-fb1cf4fc16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01878935 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01878935 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01878935]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01878935 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01878935 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01878935]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]\n",
      " [0.01028334 0.01028334 0.01028334]]\n"
     ]
    }
   ],
   "source": [
    "print(shanon_B[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f930549-b845-4513-8002-c7f041fb4afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2080, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total data for delta feature of class A and B\n",
    "deltaallA=[]\n",
    "deltaallB=[]\n",
    "for i in range(0,26):\n",
    "    deltaallA=np.append(deltaallA,deltaA[i])\n",
    "    deltaallB=np.append(deltaallB,deltaB[i])\n",
    "deltaallA=deltaallA.reshape(2080,3)\n",
    "deltaallB=deltaallB.reshape(2080,3)\n",
    "deltaallB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41714546-afd4-4322-9261-3c12239ba055",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total data of delta feature\n",
    "aDelta=np.append(deltaallA,deltaallB)\n",
    "aDelta=aDelta.reshape(4160,3)\n",
    "aDelta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "050e8544-0e71-48b3-b24c-3afeaae657f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total data for delta feature of class A and B\n",
    "thetaallA=[]\n",
    "thetaallB=[]\n",
    "for i in range(0,26):\n",
    "    thetaallA=np.append(thetaallA,thetaA[i])\n",
    "    thetaallB=np.append(thetaallB,thetaB[i])\n",
    "thetaallA=thetaallA.reshape(2080,3)\n",
    "thetaallB=thetaallB.reshape(2080,3)\n",
    "atheta=np.append(thetaallA,thetaallB)\n",
    "atheta=atheta.reshape(4160,3)\n",
    "atheta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d76c8b81-6cb8-447d-ae4b-1d4488b94774",
   "metadata": {},
   "outputs": [],
   "source": [
    "minatheta = np.min(atheta, axis=0, keepdims=True)\n",
    "maxatheta = np.max(atheta, axis=0, keepdims=True)\n",
    "atheta= (atheta- minatheta) / (maxatheta - minatheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7a70d76-b82d-4008-be1b-1a7624378c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total data for delta feature of class A and B\n",
    "alphaallA=[]\n",
    "alphaallB=[]\n",
    "for i in range(0,26):\n",
    "    alphaallA=np.append(alphaallA,alphaA[i])\n",
    "    alphaallB=np.append(alphaallB,alphaB[i])\n",
    "alphaallA=alphaallA.reshape(2080,3)\n",
    "alphaallB=alphaallB.reshape(2080,3)\n",
    "alphaAll=np.append(alphaallA,alphaallB)\n",
    "alphaAll=alphaAll.reshape(4160,3)\n",
    "alphaAll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d5ace89-2f35-4cdf-bdcf-e015f00e1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "minalphaAll = np.min(alphaAll, axis=0, keepdims=True)\n",
    "maxalphaAll = np.max(alphaAll, axis=0, keepdims=True)\n",
    "alphaAll= (alphaAll - minalphaAll) / (maxalphaAll - minalphaAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6fe69616-43eb-40ed-a5e4-aa1761d5f600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total data for delta feature of class A and B\n",
    "betaallA=[]\n",
    "betaallB=[]\n",
    "for i in range(0,26):\n",
    "    betaallA=np.append(betaallA,betaA[i])\n",
    "    betaallB=np.append(betaallB,betaB[i])\n",
    "betaallA=betaallA.reshape(2080,3)\n",
    "betaallB=betaallB.reshape(2080,3)\n",
    "abeta=np.append(betaallA,betaallB)\n",
    "abeta=abeta.reshape(4160,3)\n",
    "abeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b9a5e97-0b97-4f4f-a4be-a3206444a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "minabeta = np.min(abeta, axis=0, keepdims=True)\n",
    "maxabeta = np.max(abeta, axis=0, keepdims=True)\n",
    "abeta= (abeta - minabeta) / (maxabeta - minabeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34a51cf6-4d1d-4b17-8d59-07b050b81000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total data for delta feature of class A and B\n",
    "gammaallA=[]\n",
    "gammaallB=[]\n",
    "for i in range(0,26):\n",
    "    gammaallA=np.append(gammaallA,gammaA[i])\n",
    "    gammaallB=np.append(gammaallB,gammaB[i])\n",
    "gammaallA=gammaallA.reshape(2080,3)\n",
    "gammaallB=gammaallB.reshape(2080,3)\n",
    "agamma=np.append(gammaallA,gammaallB)\n",
    "agamma=agamma.reshape(4160,3)\n",
    "agamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0dfdf78-7153-413c-833e-1800b1cb5c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "minagamma = np.min(agamma, axis=0, keepdims=True)\n",
    "maxagamma = np.max(agamma, axis=0, keepdims=True)\n",
    "agamma= (agamma - minagamma) / (maxagamma - minagamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0eaa54c-fc33-48f4-a435-26992cbac949",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160, 9)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total data for delta feature of class A and B\n",
    "hjorthallA=[]\n",
    "hjorthallB=[]\n",
    "for i in range(0,26):\n",
    "    hjorthallA=np.append(hjorthallA,hjorthA[i])\n",
    "    hjorthallB=np.append(hjorthallB,hjorthB[i])\n",
    "hjorthallA=hjorthallA.reshape(2080,9)\n",
    "hjorthallB=hjorthallB.reshape(2080,9)\n",
    "ahjorth=np.append(hjorthallA,hjorthallB)\n",
    "ahjorth=ahjorth.reshape(4160,9)\n",
    "ahjorth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b28f6ec-efed-453b-b59f-e40d4e5b6a55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160, 6)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total data for delta feature of class A and B\n",
    "meanmedianallA=[]\n",
    "meanmedianallB=[]\n",
    "for i in range(0,26):\n",
    "    meanmedianallA=np.append(meanmedianallA,meanmedianA[i])\n",
    "    meanmedianallB=np.append(meanmedianallB,meanmedianB[i])\n",
    "meanmedianallA=meanmedianallA.reshape(2080,6)\n",
    "meanmedianallB=meanmedianallB.reshape(2080,6)\n",
    "ameanmedian=np.append(meanmedianallA,meanmedianallB)\n",
    "ameanmedian=ameanmedian.reshape(4160,6)\n",
    "ameanmedian.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d99f4e40-2f2f-4c23-8dfe-2c7ec995505e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total data for delta feature of class A and B\n",
    "shanonallA=[]\n",
    "shanonallB=[]\n",
    "for i in range(0,26):\n",
    "    shanonallA=np.append(shanonallA,shanon_A[i])\n",
    "    shanonallB=np.append(shanonallB,shanon_B[i])\n",
    "shanonallA=shanonallA.reshape(2080,3)\n",
    "shanonallB=shanonallB.reshape(2080,3)\n",
    "ashanon=np.append(shanonallA,shanonallB)\n",
    "ashanon=ashanon.reshape(4160,3)\n",
    "ashanon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "815e7a59-5b04-4111-b46d-38e58f0e2500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160, 9)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total data for delta feature of class A and B\n",
    "wavelet_featuresallA=[]\n",
    "wavelet_featuresallB=[]\n",
    "for i in range(0,26):\n",
    "    wavelet_featuresallA=np.append(wavelet_featuresallA,wavelet_features_A[i])\n",
    "    wavelet_featuresallB=np.append(wavelet_featuresallB,wavelet_features_B[i])\n",
    "wavelet_featuresallA=wavelet_featuresallA.reshape(2080,9)\n",
    "wavelet_featuresallB=wavelet_featuresallB.reshape(2080,9)\n",
    "awavelet_features=np.append(wavelet_featuresallA,wavelet_featuresallB)\n",
    "awavelet_features=awavelet_features.reshape(4160,9)\n",
    "awavelet_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e53280f4-4509-4455-ba5e-e5183c7c733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "minWavelet = np.min(awavelet_features, axis=0, keepdims=True)\n",
    "maxWavelet = np.max(awavelet_features, axis=0, keepdims=True)\n",
    "awavelet_features= (awavelet_features - minWavelet) / (maxWavelet - minWavelet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0ac7e56-3730-4f45-9864-f8bcfb4cf146",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160, 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total data for delta feature of class A and B\n",
    "skewnessallA=[]\n",
    "skewnessallB=[]\n",
    "for i in range(0,26):\n",
    "    skewnessallA=np.append(skewnessallA,skewnessA[i])\n",
    "    skewnessallB=np.append(skewnessallB,skewnessB[i])\n",
    "skewnessallA=skewnessallA.reshape(2080,3)\n",
    "skewnessallB=skewnessallB.reshape(2080,3)\n",
    "askewness=np.append(skewnessallA,skewnessallB)\n",
    "askewness=askewness.reshape(4160,3)\n",
    "askewness.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cbfbd17e-553f-45a3-b3a8-a61868b6f088",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total data for delta feature of class A and B\n",
    "kurtosisallA=[]\n",
    "kurtosisallB=[]\n",
    "for i in range(0,26):\n",
    "    kurtosisallA=np.append(kurtosisallA,kurtosisA[i])\n",
    "    kurtosisallB=np.append(kurtosisallB,kurtosisB[i])\n",
    "kurtosisallA=kurtosisallA.reshape(2080,3)\n",
    "kurtosisallB=kurtosisallB.reshape(2080,3)\n",
    "akurtosis=np.append(kurtosisallA,kurtosisallB)\n",
    "akurtosis=akurtosis.reshape(4160,3)\n",
    "akurtosis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6ef5b7a-d748-4cb3-9cc8-dc220dd73881",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160, 48)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otherData=np.column_stack((aDelta,atheta,alphaAll,abeta,agamma,ahjorth,ameanmedian,ashanon,awavelet_features,askewness, akurtosis))\n",
    "otherData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c8b0ba8-3a6e-49df-8cf0-e8354c3495ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xpother=np.empty(4160,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "40399d23-ac36-438e-8294-b3a90c8bc7d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4160,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0,2080):\n",
    "    xpother[i]=0\n",
    "for i in range(2080,4160):\n",
    "    xpother[i]=1\n",
    "xpother.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3d201d83-5fd5-47c7-ad09-ed294c9b2576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.preprocessing import StandardScaler\\nac=StandardScaler()\\nx_train=ac.fit_transform(x_train)\\nx_test=ac.transform(x_test)'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.preprocessing import StandardScaler\n",
    "ac=StandardScaler()\n",
    "x_train=ac.fit_transform(x_train)\n",
    "x_test=ac.transform(x_test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "563c1e6b-fcea-4337-b716-de493103f239",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[178  30]\n",
      " [ 49 159]]\n",
      "0.8100961538461539\n",
      "[[201   7]\n",
      " [ 57 151]]\n",
      "0.8461538461538461\n",
      "[[196  12]\n",
      " [ 54 154]]\n",
      "0.8413461538461539\n",
      "[[206   2]\n",
      " [ 62 146]]\n",
      "0.8461538461538461\n",
      "[[200   8]\n",
      " [ 58 150]]\n",
      "0.8413461538461539\n",
      "[[206   2]\n",
      " [ 63 145]]\n",
      "0.84375\n",
      "[[202   6]\n",
      " [ 57 151]]\n",
      "0.8485576923076923\n",
      "[[205   3]\n",
      " [ 65 143]]\n",
      "0.8365384615384616\n",
      "[[203   5]\n",
      " [ 61 147]]\n",
      "0.8413461538461539\n",
      "[[206   2]\n",
      " [ 63 145]]\n",
      "0.84375\n",
      "[[204   4]\n",
      " [ 61 147]]\n",
      "0.84375\n",
      "[[205   3]\n",
      " [ 65 143]]\n",
      "0.8365384615384616\n",
      "[[203   5]\n",
      " [ 62 146]]\n",
      "0.8389423076923077\n",
      "[[206   2]\n",
      " [ 65 143]]\n",
      "0.8389423076923077\n",
      "[[203   5]\n",
      " [ 63 145]]\n",
      "0.8365384615384616\n",
      "[[207   1]\n",
      " [ 64 144]]\n",
      "0.84375\n",
      "[[205   3]\n",
      " [ 63 145]]\n",
      "0.8413461538461539\n",
      "[[206   2]\n",
      " [ 64 144]]\n",
      "0.8413461538461539\n",
      "[[206   2]\n",
      " [ 64 144]]\n",
      "0.8413461538461539\n",
      "[[207   1]\n",
      " [ 64 144]]\n",
      "0.84375\n",
      "[[206   2]\n",
      " [ 63 145]]\n",
      "0.84375\n",
      "[[206   2]\n",
      " [ 64 144]]\n",
      "0.8413461538461539\n",
      "[[205   3]\n",
      " [ 64 144]]\n",
      "0.8389423076923077\n",
      "[[206   2]\n",
      " [ 64 144]]\n",
      "0.8413461538461539\n",
      "[[205   3]\n",
      " [ 63 145]]\n",
      "0.8413461538461539\n",
      "[[206   2]\n",
      " [ 65 143]]\n",
      "0.8389423076923077\n",
      "[[205   3]\n",
      " [ 65 143]]\n",
      "0.8365384615384616\n"
     ]
    }
   ],
   "source": [
    "for i in range(3,30):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier \n",
    "    x_train,x_test,y_train,y_test=train_test_split(otherData,xpother,test_size=0.1,stratify=xpother,random_state=0)\n",
    "    classknn=KNeighborsClassifier(n_neighbors=i,metric='minkowski',p=2)\n",
    "    classknn.fit(x_train,y_train)\n",
    "    y_pred=classknn.predict(x_test)\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "    cm= confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9214afa5-f39f-49d3-bb5b-fb29c8aa500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(otherData,xpother,test_size=0.1,stratify=xpother,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d6bfcc5-105b-4111-9f24-58d0bf98cb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "classknn=KNeighborsClassifier(n_neighbors=3,metric='minkowski',p=2)\n",
    "classknn.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dfb38682-21d4-4659-ba77-e3313a65a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=classknn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ec7ca0d-80b2-4ceb-94ae-3ae7ed43db65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[178  30]\n",
      " [ 49 159]]\n",
      "0.8100961538461539\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm= confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f444e5c5-08e5-4ae3-bc6e-827bfec82101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.6201923076923077\n",
      "Precision: 0.8127054052164184\n",
      "Recall: 0.8100961538461539\n",
      "F1-score: 0.8096991806363819\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "\n",
    "# Assuming you have true labels (y_true) and predicted labels (y_pred)\n",
    "# Replace y_true and y_pred with your actual data\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred, average='weighted')  # You can change the average parameter\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred, average='weighted')  # You can change the average parameter\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # You can change the average parameter\n",
    "\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Print the resultsprint(f\"Cohen's Kappa: {kappa}\")\n",
    "print(\"Cohen's Kappa:\", kappa)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "775c85db-b62d-43d2-a3d1-b48e97cbaa9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mic feature selection and checking accuracy score and confusion matrix for each no of selected feature\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "mic_scores = []\n",
    "selected_features=[]\n",
    "\n",
    "acc_scores=[]\n",
    "def calculate_MIC(x, y):\n",
    "    # Calculate Pearson correlation coefficient (r) between x and y\n",
    "    r, _ = pearsonr(x, y)\n",
    "    \n",
    "    # Calculate MIC using the formula MIC = |r| * log2(n)\n",
    "    n = len(x)\n",
    "    mic = abs(r) * np.log2(n)\n",
    "    \n",
    "    return mic\n",
    "\n",
    "for i in range(0,47):\n",
    "    #mine.compute_score(x_train[:, feature_idx], y_train)\n",
    "    mic_score = calculate_MIC(otherData[:, i], xpother)\n",
    "    #mic_score = mic(otherData[:, i], xpother)\n",
    "    mic_scores.append(mic_score)\n",
    "\n",
    "\n",
    "# Sort features based on MIC scores (in descending order)\n",
    "sorted_features = np.argsort(mic_scores)[::-1]\n",
    "\n",
    "# Select the top 20 features\n",
    "for i in range(2,47):\n",
    "    top_20_features = sorted_features[:i]\n",
    "    top_20_feature_values = otherData[:, top_20_features]\n",
    "    \n",
    "    x_train,x_test,y_train,y_test=train_test_split(top_20_feature_values,xpother,test_size=0.1,stratify=xpother,random_state=0)\n",
    "   \n",
    "    for j in range(3,20):\n",
    "        classknn=KNeighborsClassifier(n_neighbors=j,metric='minkowski',p=2)\n",
    "        classknn.fit(x_train,y_train)\n",
    "        y_pred=classknn.predict(x_test)\n",
    "        cm= confusion_matrix(y_test, y_pred)\n",
    "        acc_score=accuracy_score(y_test,y_pred)\n",
    "        acc_scores.append(acc_score)\n",
    "        #if(accuracy_score(y_test,y_pred)>0.82):\n",
    "        #print(\"No of Feature selected and neighbour selected :\",i,j)\n",
    "        #print(accuracy_score(y_test,y_pred))\n",
    "        #print(cm)\n",
    "sorted_acc_scores = np.argsort(mic_scores)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243477c-fbe4-4281-ab58-4b921bbb1d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31472562609018434,\n",
       " 0.2957354074137527,\n",
       " 0.5561643070203897,\n",
       " 0.06642356756086437,\n",
       " 0.38977663602222856,\n",
       " 0.034130294187496905,\n",
       " 0.24245122947151423,\n",
       " 0.31263126674274927,\n",
       " 0.10181718831833828,\n",
       " 0.17650982676413401,\n",
       " 0.4369779193099377,\n",
       " 0.22958782332020494,\n",
       " 0.0644707937364773,\n",
       " 0.2691548073863767,\n",
       " 0.1246047530467868,\n",
       " 7.848050500987728,\n",
       " 3.9511647590117085,\n",
       " 3.204440964920828,\n",
       " 8.203122026026477,\n",
       " 6.400225222058534,\n",
       " 5.175188398440519,\n",
       " 7.862581034228132,\n",
       " 4.518607129791445,\n",
       " 3.7017526955738322,\n",
       " 8.413634942315536,\n",
       " 8.352974548639622,\n",
       " 8.523260858391966,\n",
       " 8.493519695639991,\n",
       " 8.384354321490575,\n",
       " 8.322390651526508,\n",
       " 7.292795062108749,\n",
       " 7.4993375698754985,\n",
       " 7.422145657073783,\n",
       " 0.024978889758909342,\n",
       " 6.940417551913325,\n",
       " 6.939976188013205,\n",
       " 0.05091929339237251,\n",
       " 7.352829700224831,\n",
       " 7.352482026559234,\n",
       " 0.24645873642801305,\n",
       " 6.891042017680691,\n",
       " 6.891077320546602,\n",
       " 0.22951979072161396,\n",
       " 0.438357371852728,\n",
       " 0.29085531505679807,\n",
       " 0.4099064319123218,\n",
       " 0.4184893843355919]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mic_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "86b64818-1cad-4385-99df-80c58b9fb5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "27\n",
      "24\n",
      "28\n",
      "25\n",
      "29\n",
      "18\n",
      "21\n",
      "15\n",
      "31\n",
      "32\n",
      "37\n",
      "38\n",
      "30\n",
      "34\n",
      "35\n",
      "41\n",
      "40\n",
      "19\n",
      "20\n",
      "22\n",
      "16\n",
      "23\n",
      "17\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#Selected features after mic\n",
    "for i in range(0,25):\n",
    "    print(sorted_acc_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cc164057-df12-449b-a56b-bcc4c960502c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Feature selected :25 and no of neighbours 5\n",
      "[[189  19]\n",
      " [ 51 157]]\n",
      "0.8317307692307693\n"
     ]
    }
   ],
   "source": [
    "#Highest accuracy gained after feature selection using knn\n",
    "top_20_features = sorted_features[:25]\n",
    "top_20_feature_values = otherData[:, top_20_features]\n",
    "#column_25 = top_20_feature_values[:,24] \n",
    "#column_25=column_25.reshape(-1, 1) \n",
    "#train-test\n",
    "x_train,x_test,y_train,y_test=train_test_split(top_20_feature_values,xpother,test_size=0.1,stratify=xpother,random_state=0)\n",
    "#classification\n",
    "classknn=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\n",
    "classknn.fit(x_train,y_train)\n",
    "#prediction\n",
    "y_pred=classknn.predict(x_test)\n",
    "#confusion matrix\n",
    "cm= confusion_matrix(y_test, y_pred)\n",
    "print(\"No of Feature selected :25 and no of neighbours 5\")\n",
    "print(cm)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "38073a84-72e8-4e0e-b8dd-db27ffdbf7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 0.8245192307692307\n",
      "Fold 2 Accuracy: 0.8197115384615384\n",
      "Fold 3 Accuracy: 0.8052884615384616\n",
      "Fold 4 Accuracy: 0.8149038461538461\n",
      "Fold 5 Accuracy: 0.8100961538461539\n",
      "Fold 6 Accuracy: 0.8221153846153846\n",
      "Fold 7 Accuracy: 0.8365384615384616\n",
      "Fold 8 Accuracy: 0.7956730769230769\n",
      "Fold 9 Accuracy: 0.8485576923076923\n",
      "Fold 10 Accuracy: 0.8125\n",
      "Average Accuracy: 0.8189903846153846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming you have features (X) and labels (y)\n",
    "# X is a 2D array where each row represents a sample, and each column is a feature\n",
    "# y is a 1D array containing the labels corresponding to each sample\n",
    "\n",
    "# Define the number of splits for cross-validation\n",
    "num_splits = 10\n",
    "\n",
    "# Initialize the KFold cross-validator\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=0)\n",
    "\n",
    "# Initialize a K-Nearest Neighbors classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Standardize the features using StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# Store the accuracy scores for each fold\n",
    "accuracy_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(column_25):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = column_25[train_index],column_25[test_index]\n",
    "    y_train, y_test = xpother[train_index], xpother[test_index]\n",
    "\n",
    "    # Standardize the features (fit on training data and transform both training and testing data)\n",
    "    #X_train_std = scaler.fit_transform(X_train)\n",
    "    #X_test_std = scaler.transform(X_test)\n",
    "\n",
    "    # Train the classifier on the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    # Calculate and store the accuracy score for this fold\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "# Print the accuracy scores for each fold\n",
    "for i, acc in enumerate(accuracy_scores):\n",
    "    print(f'Fold {i+1} Accuracy: {acc}')\n",
    "\n",
    "# Calculate and print the average accuracy across all folds\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "print(f'Average Accuracy: {average_accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "916eda35-30e6-449d-b0f0-1b1afe1ffb80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa: 0.5713028576721781\n",
      "Precision: 0.7961196467121365\n",
      "Recall: 0.7860576923076923\n",
      "F1-score: 0.7839620931207432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "\n",
    "# Assuming you have true labels (y_true) and predicted labels (y_pred)\n",
    "# Replace y_true and y_pred with your actual data\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred, average='weighted')  # You can change the average parameter\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred, average='weighted')  # You can change the average parameter\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # You can change the average parameter\n",
    "\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "print(\"Cohen's Kappa:\", kappa)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\",f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
